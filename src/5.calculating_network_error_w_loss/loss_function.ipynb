{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **loss function**, also called the **cost function** is the algorithm that quantifies how wrong a model is. Loss is the measure of this metric. Since loss is the error of the model we ideally want it to be 0. We might wonder why we dont just calculate the error based on the argmax accuracy. Remember our example from earlier of confidence [0.22, 0.6, 0.18] vs [0.32, 0.36, 0.32]. If the correct class were indeed the middle one (index 1), the model accuracy would be identical between the two above. But are these two examples *really* as accurate as each other? No they are not, because accuracy is simply applying and argmax to the output to find the index of the biggest value. The output of a neural network is actually confidence, and more confidence in the correct answer is better. This is the reason we strive to increaste correct confidence and decrease misplaced confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Cross-Entropy Loss\n",
    "In linear regression there is a loss function used that is also applied in neural networks: squarred error or mean squared error with neural networks. But since we are not doing regression in our example for now we need a different loss function. Our model has a softmax activation function for the output layer, which mean it is outputting probability distribution. **Categorical cross-entropy** is explicitly used to compare a so called \"ground truth\" probability (y or \"targets) and some predicted distribution (y-hat or \"predictions\"), thus it makes sense to use cross-entropy in our cae. Is is also one of the most commonly used loss functions with softmax activation on the output layer.  \n",
    "  \n",
    "The formula for calulating the categorical cross-entropy of y (actual/desired distribution) and y-hat(predicted distribution) is:  \n",
    "Li = - sum yi,j log(^yi,j)  \n",
    "where *Li* denotes the sample loss value, *i* the i-th sample in the set, *j* is the label/output index, *y* denotes the target values and *y-hat* denotes the predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we start coding we simplify this further to *-log(correct_class_confidence)*, the formula for this is:  \n",
    "Li = -log(^y, k) , where k is an index of \"true\" probability  \n",
    "where *Li* denotes sample loss value, *i* is the i-th sample in a set, *k* is the index of the target label (ground-truth label), *y* denotes the target values and *y-hat* denotes the predicted values.  \n",
    "  \n",
    "We may ask ourselves why we call this cross-entropy and not **log loss**, which is also a type of loss. In general , the log loss error function is what we apply to the output of a binary logistic regression model (ch 16) - there are only two classes in the distribution, each of them applying to a single output (neuron) which is targeted as 0 or 1. In our case, we have a classification model that returns a probability distribution over all of the outputs. Cross-entropy compares two probability distributions. In our case, we have a softmax output of let´s say: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_output = [0.7, 0.1, 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To which probability distribution do we want to compare this to? We have 3 class confidence in the above output, let´s assume that the desired prediction is the first class (index0, which is currently 0.7). If that the intended prediciton, the desired probability distribution is [1, 0, 0]. The desired probablities will consist of a 1 in the desired class and a 0 in the remaining undesired classes. This type of arrays or vectors are called one-hot, \"hot\" is (on) with value 1 and the rest are \"cold\" (off) with values of 0. When comparing the model´s results to a one-hot vector using cross entropy, the other parts of the equation zero out and the target probability´s log loss is multiplied by , making cross-entropy calculation relatively simple. An example with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_output = [0.7, 0.1, 0.2]  \n",
    "targest = [1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the following calculations:  \n",
    "Li = - (1 * log(0.7) + 0 *log(0.1) + 0 * log(0.2)) =  \n",
    "= -(-0.3566749 + 0 + 0) = 0.3566749  \n",
    "  \n",
    "Let´s see the Python code verion of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# an example output of the output layer of the neural network\n",
    "softmax_output = [0.7, 0.1, 0.2]  \n",
    "# ground trugh\n",
    "target_output = [1, 0, 0]\n",
    "\n",
    "loss = -(math.log(softmax_output[0])*target_output[0] +\n",
    "         math.log(softmax_output[1])*target_output[1] +\n",
    "         math.log(softmax_output[2])*target_output[2])\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the full categorical cross-entropy calculation, however we can make a few assumptions given one-hot target vectors. The true values for target_output[1] and taget_output[2] are both 0 and anything multiplied by 0 is 0. So we dont need to calculate these indices. Next, what is the target_output[0], is is 1 in this case. So this can be omitted as anything multiples by 1 remains the same. The same output then in this example can be calculated with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "loss = -math.log(softmax_output[0])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can thus make some simple assumptions and use a more basic calculation, reducing it to the negative log of the target class confidence score.  \n",
    "  \n",
    "The **categorical cross-entropy loss** account for that and outputs a larger loss the lower the confidence is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "-0.05129329438755058\n",
      "-0.10536051565782628\n",
      "-0.2231435513142097\n",
      "...\n",
      "-1.6094379124341003\n",
      "-2.3025850929940455\n",
      "-2.995732273553991\n",
      "-4.605170185988091\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(math.log(1.))\n",
    "print(math.log(.95))\n",
    "print(math.log(.9))\n",
    "print(math.log(.8))\n",
    "print('...')\n",
    "print(math.log(0.2))\n",
    "print(math.log(.1))\n",
    "print(math.log(.05))\n",
    "print(math.log(.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above printed log values for a few example confidences, when the confidence level equals to 1, meaning the models is 100% \"sure\" about its prediciton, the loss value for this sample equals 0. The loss value rases with the confidence level, approaching 0. You might also wonder why we did not print the result of log(0), this is undefined. The log(x) in this book will alsays be the natural logarithm also known as ln(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.999 0.95  0.9   0.8   0.7   0.6   0.5   0.4   0.3   0.2   0.1   0.05\n",
      " 0.01 ]\n",
      "[-1.00050033e-03 -5.12932944e-02 -1.05360516e-01 -2.23143551e-01\n",
      " -3.56674944e-01 -5.10825624e-01 -6.93147181e-01 -9.16290732e-01\n",
      " -1.20397280e+00 -1.60943791e+00 -2.30258509e+00 -2.99573227e+00\n",
      " -4.60517019e+00]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OMA. Plot log of some values of x\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    #return(-math.log(x))\n",
    "    return(np.log(x))\n",
    "\n",
    "x = np.array([.999,.95,.9,.8,.7,.6,.5,.4,.3,.2,.1,.05,.01])\n",
    "y = f(x)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6486586255873816\n"
     ]
    }
   ],
   "source": [
    "# example of log\n",
    "b = 5.2\n",
    "print(np.log(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.199999999999999\n"
     ]
    }
   ],
   "source": [
    "# confirm this by exponantiating this result\n",
    "print(math.e ** 1.6486586255873816)\n",
    "\n",
    "# The reason for not exact match is floating point precision in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets condsider a neural network that does classification between three classes and the network classifies in batches o.f three. After running through the softmax activation function with a batch of 3 samples and 3 clases the output layer of the network outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities of 3 samples\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a dynamic way of calculating the categorical cross-entropy, which we know is a negative log calculation. To determin which value in the softmax output to claculate the neg log from we just need to know our target values. In this example, there are 3 classes, they could be \"dog\", \"cat\", \"human\". A doc is class 0 at index 0, cat class 1 at index 1 and human class 2 ad index 2. Let´s assume the target values are dog, cat, cat that means the list of indices woul be [0, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a list of list first to illustrate the concept\n",
    "softmax_outputs = [[0.7, 0.1, 0.2],\n",
    "                   [0.1, 0.5, 0.4],\n",
    "                   [0.02, 0.9, 0.08]]\n",
    "\n",
    "class_targets = [0, 1, 1] # dog, cat, cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First value 0 in `class_targets`means the first softmax output distribution´s intended prediciton wasthe on of the 0 index of [0.7, 0.1, 0.2] and the model has 0.7 confidence score this observation is a dog. Next in this batch of samples, in the second softmax distribution [0.1, 0.5, 0.4] was at index 2, model only has 0.5 confidence score that this is a cat, so less certain about this observation. In the last sample, is is also the 2 index from the softmax distribution a value of 0.9, so quite high confidence here. We can map softmax outputs with their intended targets to get the values from the softmax distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.5\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "for targ_idx, distribution in zip(class_targets, softmax_outputs):\n",
    "    print(distribution[targ_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `zip()` function lets us iterate over multiple iterables at the same time. But we can simplify this further using numpyy instead by creating np array of the softmax outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = [0, 1, 1]\n",
    "\n",
    "print(softmax_outputs[[0, 1, 2], class_targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values 0,1,2, numpy lets us index an array in multiple ways. We an use a list filled with indices, we could use the `class_targets` for this purpose as it already contains the list of indices that we are interested in. One problem though is that his has to filter data rows in the array, the second dimension. To perfor that, we also need to explicitly filter the array in its first dimension. This dimension contains the predicitons and we want to get them all. This we can achieve by using a list containing numbers from 0 through all of the indices. We know to have asy many indices as distributions in our whole batch so we can use the `range()` instead of typing each value ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "print(softmax_outputs\n",
    "      [range(len(softmax_outputs)), class_targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That returns the list of confidences at the target indices for each of the samples. Now we want to apply the negative log to this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n",
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "neg_log = -np.log(softmax_outputs[\n",
    "    range(len(softmax_outputs)),class_targets\n",
    "])\n",
    "\n",
    "print(neg_log)\n",
    "\n",
    "# adding average loss per batch\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also seen that targest can be one-hot encoded (OHE) where all values except one are zeros and the correct label´s position is filled with 1. They can also be sparse, which means that the numbers they contain are the correct class numbers, we are generating them in this way with the `spiral_data()` function, and we can allow the loss calculation to accept any of these forms. Since we implemented this to work with sparse labels (as in the training data) we needto add a check if they are OHE and handle this a bit differenlty in this new case. We check can be implemented by counting the dimensions - if targets are single-dimensional (lika a list), they are sparse, but if there are 2 dimensions (like a list of lists), then there is a set of one-hot encoded vectors. In this second case , we will implement a solution using the first equation fromthischapter, instead of filtering out the confidence at the target labels. We have to multiply the confidences by the targets, zerioing out all values except the ones at correct labels, performing a sum along the row axis 1. We need to a test to the ocde we just wrote for the nbr of dimensions, move the calculation of the log values outside this new if statement and implement the solution for the OHE labels following the first equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical labels shape:  1\n",
      "OHE labels shape:  2\n"
     ]
    }
   ],
   "source": [
    "# Oma example of lenght \n",
    "\n",
    "class_targets = np.array([0, 1, 1]) # categorical labels (as array, not as list)\n",
    "print(\"categorical labels shape: \", len(class_targets.shape))\n",
    "\n",
    "class_targets = np.array([[1, 0, 0], # OHE labels\n",
    "                          [0, 1, 0],\n",
    "                          [0, 1, 0]])\n",
    "print(\"OHE labels shape: \", len(class_targets.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "#class_targets = np.array([0, 1, 1])  # categorical labels  , comment out and try this one and comment out two rows below\n",
    "\n",
    "class_targets = np.array([[1, 0, 0], # OHE labels\n",
    "                          [0, 1, 0],\n",
    "                          [0, 1, 0]])\n",
    "\n",
    "# Probabilities for target values\n",
    "\n",
    "# Only if cateogorical labels   [dog, cat, cat] , [0,1,1]\n",
    "if len(class_targets.shape) == 1:\n",
    "    correct_confidences = softmax_outputs[\n",
    "        range(len(softmax_outputs)),\n",
    "        class_targets\n",
    "    ]\n",
    "\n",
    "# Mask values - only for OHE labels\n",
    "elif len(class_targets.shape) == 2:\n",
    "    correct_confidences = np.sum(\n",
    "        softmax_outputs*class_targets,\n",
    "        axis=1)\n",
    "\n",
    "# Losses\n",
    "neg_log = -np.log(correct_confidences)\n",
    "\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works, but before moving further we have an additional problem to be solved.  \n",
    "The softmax output, which is input to the loss function, consists of numbers in the range from 0 to 1, a list of confidences.. It is possible for the model to have full confidence for one label making all the remaining confidences zero. Also it is possible that the model will assign full confidence to a value that was not the target. If we try to calculate the loss of this confidence of 0 we get a problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-105-029f64eda5e8>:1: RuntimeWarning: divide by zero encountered in log\n",
      "  print(-np.log(0))\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(0))\n",
    "# The log of 0 is undefined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constant `e` to any power is always a positive number, and there is no `y` resulting from e^y = 0. This means the log(0) is undefined. We can calculate the limit of a function, but to simplify we will skip the definition of that. From w approaching 0 from positive (it is impossible to calculate the natural logarithmm of a negative value) equals negative infinity. This means that the limit is negative infinity for any infinitely small x, where x never reaches 0. In programming languages we dont have limits here, just a function which, given a parameter, returns some value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.e**(-np.inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In coding, the fewer things that are undefined, the better. We will see osme similar simplifications, when calculating derivative of absolute value function, which does not exist for an input of 0 and we have to make a decision around it. As  much as the result above of -np.log(0) would make sense, since the model would be fully wrong, this will prevent us from doing further calculations. We would also have problems later in the optimization when calculating gradients, starting with a mean value of all sample-wise lsoses since a single infinite value in a list will cause the average of that list to also be infinite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-107-c7948bb5c56b>:3: RuntimeWarning: divide by zero encountered in log\n",
      "  print(np.mean([1, 2, 3, -np.log(0)]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.mean([1, 2, 3, -np.log(0)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could **add a very small number** to the confidence to prevent it from being zero, for example 1-e7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.11809565095832\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a really small value, one tenth of a million, to the confidence at its far edge will have insignificant impact on the result. But will cause two problems, in the case when confidence is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.999999505838704e-08\n"
     ]
    }
   ],
   "source": [
    "# if confidence is 1\n",
    "print(-np.log(1+1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In his case, then model is fully correct in the prediction and all confidence is put on the correct in the label, the loss becomes negative instead of being 0. The other problem is here when shifting confidence towards 1, even if by a very small value. How to do that. To prevent these issues it is better to clip values from both sides by the same number, 1e-7 in this case. Meaning that the lowest possible value will be come 1e-7 and the highest value will become 1-1e-7 (slightly less than 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000494736474e-07\n"
     ]
    }
   ],
   "source": [
    "# This will prevent loss from being exactly 0 and making it very small instead\n",
    "print(-np.log(1-1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dont run\n",
    "\n",
    "# we acomplish this by using np.clip()\n",
    "y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `np.clip()` method performs clipping of an array of values, we we can apply it to the predicitons directly and save this as an separate array and use next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical Cross-Entropy Loss Class**  \n",
    "  \n",
    "We will be using more loss functions going ahead, but not matter which loss function we use, the overall loss is always the mean value of all sample losses. So we can create a `loss` class that contains the `calculate` method that will call our loss object´s forward method and calculate the mean value of returned sample losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "    \n",
    "    # calculate the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        \n",
    "        # calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        \n",
    "        # calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        # return loss\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert our loss class code into a class for convenience down the line to be used here and later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        # number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clip data to prevent division by 0\n",
    "        # clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # probabilities for target values -\n",
    "        # Only if cateogorical labels   [dog, cat, cat] , [0,1,1]\n",
    "        if len(class_targets.shape) == 1:\n",
    "            correct_confidences = softmax_outputs[\n",
    "                range(len(softmax_outputs)),\n",
    "                class_targets\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for OHE labels\n",
    "        elif len(class_targets.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                softmax_outputs*class_targets,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class inherist the `Loss` class and performs the error calculations that we derived in this whole chapter and can be used as an object. As an example, using the manually created outputs and targets above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate(softmax_outputs, class_targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining everything up to this point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333316 0.3333332  0.33333364]\n",
      " [0.33333287 0.3333329  0.33333418]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "loss:  1.0986104\n",
      "acc:  0.34\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "# ReLu activation\n",
    "class Activation_ReLU:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "        \n",
    "        self.output = probabilities\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "    \n",
    "    # calculate the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        \n",
    "        # calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        \n",
    "        # calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        # return loss\n",
    "        return data_loss\n",
    "\n",
    "# Cross entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        # number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clip data to prevent division by 0\n",
    "        # clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # probabilities for target values -\n",
    "        # Only if cateogorical labels   [dog, cat, cat] , [0,1,1]\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for OHE labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped*y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense Layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Denase Layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Les see output of the first few samples\n",
    "print(activation2.output[:5])\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of second dense layer here and returns loss\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "# Print loss value\n",
    "print('loss: ', loss)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# Calculate values along first axis\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "\n",
    "# Print accuracy\n",
    "print('acc: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we get ~0.33 values since our model is random, and its average loss is also not good for these data, becasue we have not yet trained our model on how to correct its errors!  \n",
    "  \n",
    "**Accuracy Calculation**  \n",
    "All though the loss is a useful metric for optimizing a model, the metric that is commonly used in practice with loss is the accuracy. This descibes how often the largest confidence is the correct class of a fraction. We can re-use existing variable definition to calculate the accuracy. We can use the argmax values from the softmax outputs and then comapre these to the targets. This is simply by slightly modifyin the `softmax_outputs` for the purpose of this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Probabilities of 3 samples\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.5, 0.1, 0.4],    # Diff here\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "# Target (ground-truth) labels for 3 samples\n",
    "class_targets = np.array([0, 1, 1])  # categorical labels  , comment out and try this one and comment out two rows below\n",
    "\n",
    "# Calculate values along second axis (axis of index 1)\n",
    "predictions = np.argmax(softmax_outputs, axis=1)\n",
    "# If targets are one-hot encoded - convert them\n",
    "if len(class_targets.shape) == 2:\n",
    "    class_targets = np.argmax(class_targets, axis=1)\n",
    "# True evaluates to 1: False to 0\n",
    "accuracy = np.mean(predictions == class_targets)\n",
    "\n",
    "print('acc: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also handling OHE targets by convering them to spare values using np.argmax(x).  \n",
    "We add the following code to the end of the script above to calculate it´s accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.34\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy from output of activation2 and targets\n",
    "# Calculate values along first axis\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "\n",
    "# Print accuracy\n",
    "print('acc: ', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
