{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducing Optimization  \n",
    "A network is now builts, we can pass data through it, we can calculate the loss, next step is to adjust the weights and biases to decrease the loss. To find an intelligent way to adjust the neurons inputÂ´s weights and biases in order to minimize the los is the main difficulty of neural networks.  \n",
    "  \n",
    "Initially one might consider option to randomly change the weights, reading the loss, repeat this untill we are happy with the lowest loss found. To demonstrate this we will use a simpler dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nnfs\n",
    "from nnfs.datasets import vertical_data\n",
    "\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = vertical_data(samples=100, classes=3)\n",
    "plt.scatter(X[:, 0], X[:, 1],\n",
    "            c=y, \n",
    "            s=40,\n",
    "            cmap='brg')\n",
    "plt.title(\"Sample Vertical Data\")\n",
    "plt.show()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code we have created previously we can use this new limited data with a simple neural network.  \n",
    "Need to run the previous full code in loss_function first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Previously written classes\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "# ReLu activation\n",
    "class Activation_ReLU:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "        \n",
    "        self.output = probabilities\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "    \n",
    "    # calculate the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        \n",
    "        # calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        \n",
    "        # calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        # return loss\n",
    "        return data_loss\n",
    "\n",
    "# Cross entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        # number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clip data to prevent division by 0\n",
    "        # clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # probabilities for target values -\n",
    "        # Only if cateogorical labels   [dog, cat, cat] , [0,1,1]\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for OHE labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped*y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code for the classes we can set up the simple neural network of for our Vertical data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "\n",
    "# Create model\n",
    "dense1 = Layer_Dense(2, 3)      # First dense layer, 2 inputs, 3 outputs\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3, 3)      # Seconde dense layer, 3 inputs, 3 outputs\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now create some variable to track the best loss and related weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper variables\n",
    "lowest_loss = 99999999 # some initial value\n",
    "best_dense1_weights = dense1.weights.copy()\n",
    "best_dense1_biases = dense1.biases.copy()\n",
    "best_dense2_weights = dense2.weights.copy()\n",
    "best_dense2_biases = dense2.biases.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialized the loss to a large value and will decrease it when a new and lower loss is found. We also copy weights and biases using `copy()` to make sure a full copy instead of reference to the object. Now we try to iterate as many times as we want, pick random values for weights and biases, then save the weights and biases if they generate the lowest-seen loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New set of weights found, iteration  0 loss:  1.1006652 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  2 loss:  1.1001756 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  3 loss:  1.0988369 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  7 loss:  1.0987284 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  11 loss:  1.0986648 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  16 loss:  1.0986221 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  21 loss:  1.09757 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  408 loss:  1.0974619 acc:  0.34\n",
      "New set of weights found, iteration  618 loss:  1.097431 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  634 loss:  1.096897 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  2463 loss:  1.0968695 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  2964 loss:  1.0968302 acc:  0.6633333333333333\n",
      "New set of weights found, iteration  4163 loss:  1.0964267 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  4471 loss:  1.096368 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  9306 loss:  1.0963554 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  13217 loss:  1.0961566 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  13857 loss:  1.095912 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  24454 loss:  1.0947107 acc:  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(100000):\n",
    "    \n",
    "    # Generate a new set of weights for iteration\n",
    "    dense1.weights = 0.05 * np.random.randn(2, 3)\n",
    "    dense1.biases = 0.05 * np.random.randn(1, 3)\n",
    "    dense2.weights = 0.05 * np.random.randn(3, 3)\n",
    "    dense2.biases = 0.05 * np.random.randn(1, 3)\n",
    "    \n",
    "    # Perform a forward pass of the training data through this layer\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # it takes the output of second dense layer here and returns loss\n",
    "    loss = loss_function.calculate(activation2.output, y)\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # Calculate values along first axis\n",
    "    predictions = np.argmax(activation2.output, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    # If loss is smaller - print and save weights and biases aside\n",
    "    if loss < lowest_loss:\n",
    "        print(\"New set of weights found, iteration \", iteration,\n",
    "              \"loss: \", loss,\n",
    "              \"acc: \", accuracy)\n",
    "        best_dense1_weights = dense1.weights.copy()\n",
    "        best_dense1_biases = dense1.biases.copy()\n",
    "        best_dense2_weights = dense2.weights.copy()\n",
    "        best_dense2_biases = dense2.biases.copy()\n",
    "        lowest_loss = loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss falls, but not by much. Accuracy did not improve, except for a singular situation where the model randomly found a set of weights yielding better accuracy. However, with a fairly large loss, this state is not stable. We can conclude that this does not appear to be a reliable method for minimizing loss. After running for 1 billion of iterations (from the book), the best results with lowest loss was: `loss: 1.0911305 acc: 0.333333333`  \n",
    "  \n",
    "Even with this basic dataset, we see that randomly searching for weight and bias combinations will take far too long to be an acceptable method. Another idea could be, instead of setting parameters with randomly chosen values each iteration, apply a fraction of these values to parameters. With this, weights will be updated from what currently yields us the lowest loss instead of aimlessly randomly. If the adjustment decreases loss, we make it the new point to adjust from. If loss instead increases due to adjustment, then we will revert to previous point. Using similar code from earliers, we will first change from randomly selecting weights and biases to randomly *adjusting* these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Udate weights with some small random values\n",
    "dense1.weights += 0.05 * np.random.randn(2, 3)\n",
    "dense1.biases += 0.05 * np.random.randn(1, 3)\n",
    "dense2.weights += 0.05 * np.random.randn(3, 3)\n",
    "dense2.biases += 0.05 * np.random.randn(1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will change our ending `if` statement to be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if los is smaller - print and save weights and biases aside\n",
    "if loss < lowest_loss:\n",
    "    print(\"New set of weights found, iteration \", iteration,\n",
    "              \"loss: \", loss,\n",
    "              \"acc: \", accuracy)\n",
    "    best_dense1_weights = dense1.weights.copy()\n",
    "    best_dense1_biases = dense1.biases.copy()\n",
    "    best_dense2_weights = dense2.weights.copy()\n",
    "    best_dense2_biases = dense2.biases.copy()\n",
    "    lowest_loss = loss\n",
    "# Revert weights and biases\n",
    "else:\n",
    "    dense1.weights = best_dense1_weights.copy()\n",
    "    dense1.biases = best_dense1_biases.copy()\n",
    "    dense2.weights = best_dense2_weights.copy()\n",
    "    dense2.biases = best_dense2_biases.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full code up to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New set of weights found, iteration  0 loss:  1.098234 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  8 loss:  1.0979 acc:  0.5933333333333334\n",
      "New set of weights found, iteration  20 loss:  1.0974861 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  21 loss:  1.0971179 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  22 loss:  1.0957131 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  23 loss:  1.094928 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  26 loss:  1.093027 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  30 loss:  1.091621 acc:  0.5533333333333333\n",
      "New set of weights found, iteration  33 loss:  1.0916046 acc:  0.49666666666666665\n",
      "New set of weights found, iteration  34 loss:  1.085297 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  36 loss:  1.0748203 acc:  0.36\n",
      "New set of weights found, iteration  40 loss:  1.0705974 acc:  0.5333333333333333\n",
      "New set of weights found, iteration  51 loss:  1.0687438 acc:  0.66\n",
      "New set of weights found, iteration  54 loss:  1.0674101 acc:  0.47\n",
      "New set of weights found, iteration  55 loss:  1.0662465 acc:  0.5766666666666667\n",
      "New set of weights found, iteration  56 loss:  1.0630295 acc:  0.6666666666666666\n",
      "New set of weights found, iteration  57 loss:  1.0616355 acc:  0.5166666666666667\n",
      "New set of weights found, iteration  59 loss:  1.0602895 acc:  0.4\n",
      "New set of weights found, iteration  63 loss:  1.0524381 acc:  0.37333333333333335\n",
      "New set of weights found, iteration  66 loss:  1.0507361 acc:  0.59\n",
      "New set of weights found, iteration  67 loss:  1.0387 acc:  0.5733333333333334\n",
      "New set of weights found, iteration  70 loss:  1.0355068 acc:  0.42333333333333334\n",
      "New set of weights found, iteration  73 loss:  1.0320003 acc:  0.5333333333333333\n",
      "New set of weights found, iteration  79 loss:  1.0190774 acc:  0.6666666666666666\n",
      "New set of weights found, iteration  81 loss:  1.0183512 acc:  0.6666666666666666\n",
      "New set of weights found, iteration  82 loss:  1.0099083 acc:  0.66\n",
      "New set of weights found, iteration  85 loss:  1.0009966 acc:  0.6533333333333333\n",
      "New set of weights found, iteration  90 loss:  0.99192727 acc:  0.6666666666666666\n",
      "New set of weights found, iteration  91 loss:  0.9851257 acc:  0.6666666666666666\n",
      "New set of weights found, iteration  94 loss:  0.9695329 acc:  0.7766666666666666\n",
      "New set of weights found, iteration  95 loss:  0.9689904 acc:  0.6733333333333333\n",
      "New set of weights found, iteration  96 loss:  0.96338797 acc:  0.6633333333333333\n",
      "New set of weights found, iteration  97 loss:  0.96204877 acc:  0.6666666666666666\n",
      "New set of weights found, iteration  98 loss:  0.9603791 acc:  0.6633333333333333\n",
      "New set of weights found, iteration  99 loss:  0.9549933 acc:  0.6733333333333333\n",
      "New set of weights found, iteration  100 loss:  0.941937 acc:  0.67\n",
      "New set of weights found, iteration  101 loss:  0.9366995 acc:  0.6733333333333333\n",
      "New set of weights found, iteration  102 loss:  0.9360379 acc:  0.6666666666666666\n",
      "New set of weights found, iteration  104 loss:  0.92980856 acc:  0.69\n",
      "New set of weights found, iteration  106 loss:  0.9118104 acc:  0.6666666666666666\n",
      "New set of weights found, iteration  111 loss:  0.90839577 acc:  0.67\n",
      "New set of weights found, iteration  115 loss:  0.90156597 acc:  0.6633333333333333\n",
      "New set of weights found, iteration  119 loss:  0.888611 acc:  0.6566666666666666\n",
      "New set of weights found, iteration  120 loss:  0.8711584 acc:  0.68\n",
      "New set of weights found, iteration  121 loss:  0.8695929 acc:  0.7166666666666667\n",
      "New set of weights found, iteration  123 loss:  0.8643388 acc:  0.6866666666666666\n",
      "New set of weights found, iteration  124 loss:  0.8574512 acc:  0.6633333333333333\n",
      "New set of weights found, iteration  128 loss:  0.8509625 acc:  0.6466666666666666\n",
      "New set of weights found, iteration  131 loss:  0.84873813 acc:  0.6566666666666666\n",
      "New set of weights found, iteration  134 loss:  0.8464279 acc:  0.6533333333333333\n",
      "New set of weights found, iteration  139 loss:  0.83442646 acc:  0.6533333333333333\n",
      "New set of weights found, iteration  140 loss:  0.8289818 acc:  0.6633333333333333\n",
      "New set of weights found, iteration  142 loss:  0.815896 acc:  0.6666666666666666\n",
      "New set of weights found, iteration  144 loss:  0.8075986 acc:  0.68\n",
      "New set of weights found, iteration  147 loss:  0.7913044 acc:  0.6733333333333333\n",
      "New set of weights found, iteration  148 loss:  0.77926016 acc:  0.7333333333333333\n",
      "New set of weights found, iteration  155 loss:  0.7782132 acc:  0.72\n",
      "New set of weights found, iteration  157 loss:  0.7768326 acc:  0.77\n",
      "New set of weights found, iteration  160 loss:  0.7731794 acc:  0.6566666666666666\n",
      "New set of weights found, iteration  161 loss:  0.77065897 acc:  0.66\n",
      "New set of weights found, iteration  162 loss:  0.76416343 acc:  0.69\n",
      "New set of weights found, iteration  164 loss:  0.75933164 acc:  0.8266666666666667\n",
      "New set of weights found, iteration  166 loss:  0.7546425 acc:  0.7033333333333334\n",
      "New set of weights found, iteration  168 loss:  0.75352114 acc:  0.68\n",
      "New set of weights found, iteration  169 loss:  0.74634296 acc:  0.7933333333333333\n",
      "New set of weights found, iteration  172 loss:  0.745082 acc:  0.7833333333333333\n",
      "New set of weights found, iteration  174 loss:  0.73841715 acc:  0.72\n",
      "New set of weights found, iteration  176 loss:  0.7315821 acc:  0.7333333333333333\n",
      "New set of weights found, iteration  178 loss:  0.7314743 acc:  0.67\n",
      "New set of weights found, iteration  179 loss:  0.72053915 acc:  0.6633333333333333\n",
      "New set of weights found, iteration  181 loss:  0.71018237 acc:  0.66\n",
      "New set of weights found, iteration  183 loss:  0.70639414 acc:  0.6833333333333333\n",
      "New set of weights found, iteration  184 loss:  0.6959206 acc:  0.67\n",
      "New set of weights found, iteration  190 loss:  0.68638885 acc:  0.7166666666666667\n",
      "New set of weights found, iteration  197 loss:  0.68288887 acc:  0.76\n",
      "New set of weights found, iteration  201 loss:  0.6811016 acc:  0.8966666666666666\n",
      "New set of weights found, iteration  202 loss:  0.67029 acc:  0.8133333333333334\n",
      "New set of weights found, iteration  204 loss:  0.6638352 acc:  0.87\n",
      "New set of weights found, iteration  206 loss:  0.6500706 acc:  0.84\n",
      "New set of weights found, iteration  208 loss:  0.64916474 acc:  0.8633333333333333\n",
      "New set of weights found, iteration  210 loss:  0.6247521 acc:  0.87\n",
      "New set of weights found, iteration  223 loss:  0.62182105 acc:  0.83\n",
      "New set of weights found, iteration  225 loss:  0.61474174 acc:  0.8133333333333334\n",
      "New set of weights found, iteration  229 loss:  0.6098919 acc:  0.79\n",
      "New set of weights found, iteration  232 loss:  0.6070641 acc:  0.84\n",
      "New set of weights found, iteration  235 loss:  0.5966077 acc:  0.8433333333333334\n",
      "New set of weights found, iteration  236 loss:  0.5912133 acc:  0.8833333333333333\n",
      "New set of weights found, iteration  239 loss:  0.5809289 acc:  0.8833333333333333\n",
      "New set of weights found, iteration  245 loss:  0.57965416 acc:  0.8333333333333334\n",
      "New set of weights found, iteration  247 loss:  0.5664641 acc:  0.89\n",
      "New set of weights found, iteration  251 loss:  0.5619944 acc:  0.8666666666666667\n",
      "New set of weights found, iteration  252 loss:  0.55908585 acc:  0.89\n",
      "New set of weights found, iteration  254 loss:  0.5573708 acc:  0.8466666666666667\n",
      "New set of weights found, iteration  257 loss:  0.5461338 acc:  0.8733333333333333\n",
      "New set of weights found, iteration  260 loss:  0.5443029 acc:  0.8833333333333333\n",
      "New set of weights found, iteration  262 loss:  0.5414836 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  263 loss:  0.5346275 acc:  0.8966666666666666\n",
      "New set of weights found, iteration  264 loss:  0.5280197 acc:  0.9\n",
      "New set of weights found, iteration  265 loss:  0.5220954 acc:  0.89\n",
      "New set of weights found, iteration  267 loss:  0.52021086 acc:  0.89\n",
      "New set of weights found, iteration  268 loss:  0.51590306 acc:  0.8866666666666667\n",
      "New set of weights found, iteration  271 loss:  0.49162588 acc:  0.8866666666666667\n",
      "New set of weights found, iteration  275 loss:  0.49036554 acc:  0.8766666666666667\n",
      "New set of weights found, iteration  281 loss:  0.47586605 acc:  0.88\n",
      "New set of weights found, iteration  285 loss:  0.46719283 acc:  0.8733333333333333\n",
      "New set of weights found, iteration  287 loss:  0.46274963 acc:  0.9\n",
      "New set of weights found, iteration  290 loss:  0.46223977 acc:  0.8933333333333333\n",
      "New set of weights found, iteration  294 loss:  0.45942557 acc:  0.8866666666666667\n",
      "New set of weights found, iteration  295 loss:  0.4410697 acc:  0.8966666666666666\n",
      "New set of weights found, iteration  296 loss:  0.4393491 acc:  0.89\n",
      "New set of weights found, iteration  301 loss:  0.42511973 acc:  0.9\n",
      "New set of weights found, iteration  306 loss:  0.42060673 acc:  0.8966666666666666\n",
      "New set of weights found, iteration  310 loss:  0.41993254 acc:  0.8966666666666666\n",
      "New set of weights found, iteration  318 loss:  0.4187191 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  321 loss:  0.40812382 acc:  0.9\n",
      "New set of weights found, iteration  324 loss:  0.40176314 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  331 loss:  0.39882812 acc:  0.8966666666666666\n",
      "New set of weights found, iteration  336 loss:  0.39756805 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  341 loss:  0.39669657 acc:  0.8966666666666666\n",
      "New set of weights found, iteration  351 loss:  0.389931 acc:  0.8933333333333333\n",
      "New set of weights found, iteration  353 loss:  0.37798747 acc:  0.9\n",
      "New set of weights found, iteration  354 loss:  0.3698993 acc:  0.9\n",
      "New set of weights found, iteration  355 loss:  0.368832 acc:  0.8966666666666666\n",
      "New set of weights found, iteration  356 loss:  0.36205336 acc:  0.9\n",
      "New set of weights found, iteration  361 loss:  0.35878137 acc:  0.8866666666666667\n",
      "New set of weights found, iteration  382 loss:  0.35736766 acc:  0.9\n",
      "New set of weights found, iteration  395 loss:  0.3483527 acc:  0.8933333333333333\n",
      "New set of weights found, iteration  401 loss:  0.3411524 acc:  0.8933333333333333\n",
      "New set of weights found, iteration  405 loss:  0.3390277 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  415 loss:  0.33324108 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  416 loss:  0.32446328 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  420 loss:  0.32253957 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  421 loss:  0.32195604 acc:  0.89\n",
      "New set of weights found, iteration  423 loss:  0.31690094 acc:  0.8866666666666667\n",
      "New set of weights found, iteration  431 loss:  0.31493682 acc:  0.89\n",
      "New set of weights found, iteration  441 loss:  0.30988634 acc:  0.8966666666666666\n",
      "New set of weights found, iteration  444 loss:  0.3085328 acc:  0.8966666666666666\n",
      "New set of weights found, iteration  448 loss:  0.30640945 acc:  0.89\n",
      "New set of weights found, iteration  455 loss:  0.30509567 acc:  0.9\n",
      "New set of weights found, iteration  457 loss:  0.30329996 acc:  0.9\n",
      "New set of weights found, iteration  460 loss:  0.30136317 acc:  0.8933333333333333\n",
      "New set of weights found, iteration  461 loss:  0.30075645 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  476 loss:  0.30044773 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  479 loss:  0.2985783 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  480 loss:  0.29798824 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  487 loss:  0.29710048 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  503 loss:  0.29390934 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  508 loss:  0.2893733 acc:  0.9\n",
      "New set of weights found, iteration  512 loss:  0.2893473 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  522 loss:  0.28913173 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  525 loss:  0.28756455 acc:  0.8933333333333333\n",
      "New set of weights found, iteration  526 loss:  0.2861331 acc:  0.8933333333333333\n",
      "New set of weights found, iteration  528 loss:  0.28326604 acc:  0.8966666666666666\n",
      "New set of weights found, iteration  532 loss:  0.28203225 acc:  0.8933333333333333\n",
      "New set of weights found, iteration  536 loss:  0.28053796 acc:  0.89\n",
      "New set of weights found, iteration  543 loss:  0.2767067 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  547 loss:  0.27506837 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  552 loss:  0.27496597 acc:  0.8933333333333333\n",
      "New set of weights found, iteration  553 loss:  0.27358317 acc:  0.91\n",
      "New set of weights found, iteration  557 loss:  0.27123138 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  558 loss:  0.26796523 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  570 loss:  0.26776564 acc:  0.91\n",
      "New set of weights found, iteration  571 loss:  0.2669823 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  573 loss:  0.26684746 acc:  0.8933333333333333\n",
      "New set of weights found, iteration  581 loss:  0.2666228 acc:  0.9\n",
      "New set of weights found, iteration  588 loss:  0.26548538 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  593 loss:  0.26430085 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  597 loss:  0.26378208 acc:  0.91\n",
      "New set of weights found, iteration  598 loss:  0.26299793 acc:  0.8933333333333333\n",
      "New set of weights found, iteration  599 loss:  0.26216096 acc:  0.91\n",
      "New set of weights found, iteration  611 loss:  0.26030856 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  612 loss:  0.25857484 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  616 loss:  0.25845724 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  629 loss:  0.25794291 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  633 loss:  0.25728297 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  641 loss:  0.25531822 acc:  0.91\n",
      "New set of weights found, iteration  649 loss:  0.25530303 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  652 loss:  0.25488228 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  655 loss:  0.25459436 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  662 loss:  0.25456226 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  667 loss:  0.25273636 acc:  0.8966666666666666\n",
      "New set of weights found, iteration  674 loss:  0.2524969 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  686 loss:  0.25112188 acc:  0.9\n",
      "New set of weights found, iteration  690 loss:  0.25108775 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  694 loss:  0.24900569 acc:  0.8966666666666666\n",
      "New set of weights found, iteration  712 loss:  0.24798961 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  729 loss:  0.24658407 acc:  0.9\n",
      "New set of weights found, iteration  731 loss:  0.24565841 acc:  0.9\n",
      "New set of weights found, iteration  732 loss:  0.2450973 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  734 loss:  0.24392276 acc:  0.8966666666666666\n",
      "New set of weights found, iteration  741 loss:  0.2437223 acc:  0.9\n",
      "New set of weights found, iteration  745 loss:  0.24336591 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  784 loss:  0.24331276 acc:  0.91\n",
      "New set of weights found, iteration  793 loss:  0.24307637 acc:  0.9\n",
      "New set of weights found, iteration  796 loss:  0.24284002 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  803 loss:  0.24209443 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  826 loss:  0.24174964 acc:  0.9\n",
      "New set of weights found, iteration  871 loss:  0.2405723 acc:  0.91\n",
      "New set of weights found, iteration  896 loss:  0.24025431 acc:  0.9\n",
      "New set of weights found, iteration  1050 loss:  0.23983318 acc:  0.9033333333333333\n",
      "New set of weights found, iteration  1093 loss:  0.23972076 acc:  0.91\n",
      "New set of weights found, iteration  1098 loss:  0.2389593 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  1103 loss:  0.23885892 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  1197 loss:  0.23865783 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  1291 loss:  0.23854573 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  1834 loss:  0.23854457 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  1935 loss:  0.23841359 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  2327 loss:  0.23823756 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  2363 loss:  0.23819053 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  2379 loss:  0.23817469 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  2532 loss:  0.23814255 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  2850 loss:  0.23809181 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  2955 loss:  0.23795557 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  3491 loss:  0.23785743 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  4083 loss:  0.23777185 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  4598 loss:  0.23767301 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  13720 loss:  0.2376649 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  14991 loss:  0.23763071 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  16292 loss:  0.23762736 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  22027 loss:  0.23760732 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  22756 loss:  0.23759425 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  22997 loss:  0.23752756 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  24408 loss:  0.23745723 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  24905 loss:  0.23745285 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  25006 loss:  0.23745048 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  25524 loss:  0.23743479 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  28135 loss:  0.2374144 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  28815 loss:  0.23738988 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  28841 loss:  0.2373785 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  29095 loss:  0.23732156 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  38488 loss:  0.23730786 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  39178 loss:  0.23730695 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  39639 loss:  0.23725344 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  44359 loss:  0.2372435 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  52218 loss:  0.2372398 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  52770 loss:  0.23720036 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  57988 loss:  0.23719986 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  62706 loss:  0.23718351 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  65134 loss:  0.23717794 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  65402 loss:  0.23715526 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  75403 loss:  0.23712893 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  83293 loss:  0.23710866 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  88344 loss:  0.23709041 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  89467 loss:  0.23708487 acc:  0.9066666666666666\n",
      "New set of weights found, iteration  95202 loss:  0.2370694 acc:  0.9066666666666666\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "\n",
    "# Create model\n",
    "dense1 = Layer_Dense(2, 3)      # First dense layer, 2 inputs, 3 outputs\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3, 3)      # Seconde dense layer, 3 inputs, 3 outputs\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Helper variables\n",
    "lowest_loss = 99999999 # some initial value\n",
    "best_dense1_weights = dense1.weights.copy()\n",
    "best_dense1_biases = dense1.biases.copy()\n",
    "best_dense2_weights = dense2.weights.copy()\n",
    "best_dense2_biases = dense2.biases.copy()\n",
    "\n",
    "for iteration in range(100000):\n",
    "    \n",
    "    # Generate a new set of weights for iteration\n",
    "    dense1.weights += 0.05 * np.random.randn(2, 3)\n",
    "    dense1.biases += 0.05 * np.random.randn(1, 3)\n",
    "    dense2.weights += 0.05 * np.random.randn(3, 3)\n",
    "    dense2.biases += 0.05 * np.random.randn(1, 3)\n",
    "    \n",
    "    # Perform a forward pass of the training data through this layer\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # it takes the output of second dense layer here and returns loss\n",
    "    loss = loss_function.calculate(activation2.output, y)\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # Calculate values along first axis\n",
    "    predictions = np.argmax(activation2.output, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    # if los is smaller - print and save weights and biases aside\n",
    "    if loss < lowest_loss:\n",
    "        print(\"New set of weights found, iteration \", iteration,\n",
    "                  \"loss: \", loss,\n",
    "                  \"acc: \", accuracy)\n",
    "        best_dense1_weights = dense1.weights.copy()\n",
    "        best_dense1_biases = dense1.biases.copy()\n",
    "        best_dense2_weights = dense2.weights.copy()\n",
    "        best_dense2_biases = dense2.biases.copy()\n",
    "        lowest_loss = loss\n",
    "# Revert weights and biases\n",
    "    else:\n",
    "        dense1.weights = best_dense1_weights.copy()\n",
    "        dense1.biases = best_dense1_biases.copy()\n",
    "        dense2.weights = best_dense2_weights.copy()\n",
    "        dense2.biases = best_dense2_biases.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss descended by a descent amount this time, and accuracy raised significantly. Applying a fraction of random values actually lead to a result that we could almost call some sort of solution. If you try 100 000 iterations, you will not progress much further.  \n",
    "  \n",
    "LetÂ´s try it again with the *spiral dataset* instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New set of weights found, iteration  0 loss:  1.0989411 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  2 loss:  1.098805 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  3 loss:  1.098068 acc:  0.3333333333333333\n",
      "New set of weights found, iteration  16 loss:  1.0977746 acc:  0.33666666666666667\n",
      "New set of weights found, iteration  20 loss:  1.0977545 acc:  0.3566666666666667\n",
      "New set of weights found, iteration  24 loss:  1.0974311 acc:  0.31\n",
      "New set of weights found, iteration  33 loss:  1.0972637 acc:  0.32\n",
      "New set of weights found, iteration  35 loss:  1.0958542 acc:  0.33\n",
      "New set of weights found, iteration  48 loss:  1.0957701 acc:  0.35333333333333333\n",
      "New set of weights found, iteration  52 loss:  1.0957057 acc:  0.34\n",
      "New set of weights found, iteration  53 loss:  1.0940795 acc:  0.34\n",
      "New set of weights found, iteration  55 loss:  1.0935097 acc:  0.32666666666666666\n",
      "New set of weights found, iteration  62 loss:  1.0933009 acc:  0.3433333333333333\n",
      "New set of weights found, iteration  65 loss:  1.0924752 acc:  0.37333333333333335\n",
      "New set of weights found, iteration  66 loss:  1.0924345 acc:  0.33\n",
      "New set of weights found, iteration  69 loss:  1.0914644 acc:  0.35\n",
      "New set of weights found, iteration  70 loss:  1.0907152 acc:  0.36333333333333334\n",
      "New set of weights found, iteration  71 loss:  1.0901017 acc:  0.35\n",
      "New set of weights found, iteration  72 loss:  1.0887939 acc:  0.38333333333333336\n",
      "New set of weights found, iteration  73 loss:  1.0885375 acc:  0.4066666666666667\n",
      "New set of weights found, iteration  76 loss:  1.0882773 acc:  0.38\n",
      "New set of weights found, iteration  78 loss:  1.0872074 acc:  0.37333333333333335\n",
      "New set of weights found, iteration  92 loss:  1.0868285 acc:  0.38\n",
      "New set of weights found, iteration  98 loss:  1.0864158 acc:  0.39\n",
      "New set of weights found, iteration  99 loss:  1.0859452 acc:  0.42333333333333334\n",
      "New set of weights found, iteration  102 loss:  1.085752 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  103 loss:  1.0856608 acc:  0.37\n",
      "New set of weights found, iteration  104 loss:  1.085151 acc:  0.35333333333333333\n",
      "New set of weights found, iteration  105 loss:  1.0849135 acc:  0.36666666666666664\n",
      "New set of weights found, iteration  108 loss:  1.0844669 acc:  0.38\n",
      "New set of weights found, iteration  111 loss:  1.0844182 acc:  0.42\n",
      "New set of weights found, iteration  116 loss:  1.0834806 acc:  0.41\n",
      "New set of weights found, iteration  118 loss:  1.0832461 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  120 loss:  1.0829114 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  123 loss:  1.0826646 acc:  0.37666666666666665\n",
      "New set of weights found, iteration  124 loss:  1.0804119 acc:  0.37333333333333335\n",
      "New set of weights found, iteration  141 loss:  1.0801297 acc:  0.4166666666666667\n",
      "New set of weights found, iteration  159 loss:  1.0800773 acc:  0.41\n",
      "New set of weights found, iteration  188 loss:  1.0797122 acc:  0.3933333333333333\n",
      "New set of weights found, iteration  189 loss:  1.0796055 acc:  0.37666666666666665\n",
      "New set of weights found, iteration  206 loss:  1.0795181 acc:  0.37666666666666665\n",
      "New set of weights found, iteration  225 loss:  1.0794258 acc:  0.4166666666666667\n",
      "New set of weights found, iteration  232 loss:  1.0792458 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  345 loss:  1.0791955 acc:  0.4033333333333333\n",
      "New set of weights found, iteration  357 loss:  1.0791788 acc:  0.4066666666666667\n",
      "New set of weights found, iteration  448 loss:  1.0788352 acc:  0.38333333333333336\n",
      "New set of weights found, iteration  462 loss:  1.0787933 acc:  0.41\n",
      "New set of weights found, iteration  463 loss:  1.078743 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  471 loss:  1.0785816 acc:  0.41333333333333333\n",
      "New set of weights found, iteration  524 loss:  1.0782448 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  532 loss:  1.0779909 acc:  0.39\n",
      "New set of weights found, iteration  542 loss:  1.077732 acc:  0.41\n",
      "New set of weights found, iteration  543 loss:  1.0770626 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  677 loss:  1.0768765 acc:  0.4166666666666667\n",
      "New set of weights found, iteration  719 loss:  1.0767819 acc:  0.38333333333333336\n",
      "New set of weights found, iteration  721 loss:  1.0767174 acc:  0.4033333333333333\n",
      "New set of weights found, iteration  913 loss:  1.0767118 acc:  0.38333333333333336\n",
      "New set of weights found, iteration  950 loss:  1.076248 acc:  0.37333333333333335\n",
      "New set of weights found, iteration  970 loss:  1.0756332 acc:  0.41\n",
      "New set of weights found, iteration  1037 loss:  1.0754604 acc:  0.38333333333333336\n",
      "New set of weights found, iteration  1150 loss:  1.075396 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  1152 loss:  1.0753101 acc:  0.38333333333333336\n",
      "New set of weights found, iteration  1202 loss:  1.0752207 acc:  0.3933333333333333\n",
      "New set of weights found, iteration  1208 loss:  1.0751264 acc:  0.43333333333333335\n",
      "New set of weights found, iteration  1225 loss:  1.0750334 acc:  0.4066666666666667\n",
      "New set of weights found, iteration  1233 loss:  1.0742704 acc:  0.4\n",
      "New set of weights found, iteration  1240 loss:  1.0741962 acc:  0.41\n",
      "New set of weights found, iteration  1257 loss:  1.0738564 acc:  0.41\n",
      "New set of weights found, iteration  1269 loss:  1.0736086 acc:  0.4066666666666667\n",
      "New set of weights found, iteration  1282 loss:  1.0731966 acc:  0.3933333333333333\n",
      "New set of weights found, iteration  1283 loss:  1.0726882 acc:  0.4033333333333333\n",
      "New set of weights found, iteration  1300 loss:  1.0719748 acc:  0.43666666666666665\n",
      "New set of weights found, iteration  1307 loss:  1.0718962 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  1445 loss:  1.0713644 acc:  0.4\n",
      "New set of weights found, iteration  1503 loss:  1.0710763 acc:  0.4033333333333333\n",
      "New set of weights found, iteration  1510 loss:  1.0707201 acc:  0.3933333333333333\n",
      "New set of weights found, iteration  1521 loss:  1.0705312 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  1535 loss:  1.0696213 acc:  0.4033333333333333\n",
      "New set of weights found, iteration  1585 loss:  1.069202 acc:  0.4\n",
      "New set of weights found, iteration  1693 loss:  1.0691624 acc:  0.4166666666666667\n",
      "New set of weights found, iteration  1716 loss:  1.0688572 acc:  0.42\n",
      "New set of weights found, iteration  1752 loss:  1.0686456 acc:  0.4\n",
      "New set of weights found, iteration  1803 loss:  1.0680292 acc:  0.45666666666666667\n",
      "New set of weights found, iteration  1879 loss:  1.0676646 acc:  0.4033333333333333\n",
      "New set of weights found, iteration  1883 loss:  1.0671419 acc:  0.42333333333333334\n",
      "New set of weights found, iteration  1921 loss:  1.0667741 acc:  0.39\n",
      "New set of weights found, iteration  1940 loss:  1.0664172 acc:  0.41\n",
      "New set of weights found, iteration  1968 loss:  1.0664052 acc:  0.41\n",
      "New set of weights found, iteration  1987 loss:  1.0662199 acc:  0.4266666666666667\n",
      "New set of weights found, iteration  1998 loss:  1.066177 acc:  0.42\n",
      "New set of weights found, iteration  2005 loss:  1.0659565 acc:  0.4066666666666667\n",
      "New set of weights found, iteration  2007 loss:  1.0657188 acc:  0.43\n",
      "New set of weights found, iteration  2021 loss:  1.0656891 acc:  0.39\n",
      "New set of weights found, iteration  2026 loss:  1.0646366 acc:  0.43333333333333335\n",
      "New set of weights found, iteration  2034 loss:  1.0645478 acc:  0.43333333333333335\n",
      "New set of weights found, iteration  2091 loss:  1.0638926 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  2101 loss:  1.0635463 acc:  0.4166666666666667\n",
      "New set of weights found, iteration  2102 loss:  1.0630233 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  2120 loss:  1.062853 acc:  0.41333333333333333\n",
      "New set of weights found, iteration  2126 loss:  1.0626475 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  2131 loss:  1.0620239 acc:  0.39\n",
      "New set of weights found, iteration  2134 loss:  1.0617068 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  2139 loss:  1.0613177 acc:  0.4266666666666667\n",
      "New set of weights found, iteration  2140 loss:  1.0609627 acc:  0.41333333333333333\n",
      "New set of weights found, iteration  2151 loss:  1.060039 acc:  0.3933333333333333\n",
      "New set of weights found, iteration  2329 loss:  1.0597776 acc:  0.4033333333333333\n",
      "New set of weights found, iteration  2375 loss:  1.059599 acc:  0.38333333333333336\n",
      "New set of weights found, iteration  2439 loss:  1.0594449 acc:  0.39\n",
      "New set of weights found, iteration  2489 loss:  1.0594049 acc:  0.4066666666666667\n",
      "New set of weights found, iteration  2506 loss:  1.059365 acc:  0.41\n",
      "New set of weights found, iteration  2511 loss:  1.0590916 acc:  0.4066666666666667\n",
      "New set of weights found, iteration  2528 loss:  1.0590004 acc:  0.42\n",
      "New set of weights found, iteration  2530 loss:  1.0588315 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  2534 loss:  1.058569 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  2546 loss:  1.05851 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  2567 loss:  1.0584509 acc:  0.39\n",
      "New set of weights found, iteration  2616 loss:  1.0583087 acc:  0.39\n",
      "New set of weights found, iteration  2636 loss:  1.058183 acc:  0.42\n",
      "New set of weights found, iteration  2670 loss:  1.0581824 acc:  0.4266666666666667\n",
      "New set of weights found, iteration  2673 loss:  1.0580996 acc:  0.4066666666666667\n",
      "New set of weights found, iteration  2742 loss:  1.0580208 acc:  0.4166666666666667\n",
      "New set of weights found, iteration  2781 loss:  1.0580163 acc:  0.38\n",
      "New set of weights found, iteration  2817 loss:  1.0577513 acc:  0.42333333333333334\n",
      "New set of weights found, iteration  2840 loss:  1.0576515 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  2857 loss:  1.0574597 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  2859 loss:  1.0565815 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  2875 loss:  1.0565193 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  3130 loss:  1.0561845 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  3360 loss:  1.0561334 acc:  0.3933333333333333\n",
      "New set of weights found, iteration  3707 loss:  1.0560187 acc:  0.4033333333333333\n",
      "New set of weights found, iteration  3805 loss:  1.0555923 acc:  0.4033333333333333\n",
      "New set of weights found, iteration  3875 loss:  1.055422 acc:  0.43333333333333335\n",
      "New set of weights found, iteration  4068 loss:  1.0552936 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  4106 loss:  1.0550165 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  4186 loss:  1.0549904 acc:  0.39\n",
      "New set of weights found, iteration  4247 loss:  1.0547156 acc:  0.4\n",
      "New set of weights found, iteration  4376 loss:  1.0546837 acc:  0.4066666666666667\n",
      "New set of weights found, iteration  4423 loss:  1.0545644 acc:  0.42\n",
      "New set of weights found, iteration  4522 loss:  1.0545043 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  4529 loss:  1.0543778 acc:  0.3933333333333333\n",
      "New set of weights found, iteration  4535 loss:  1.0540115 acc:  0.3933333333333333\n",
      "New set of weights found, iteration  5068 loss:  1.0539833 acc:  0.39\n",
      "New set of weights found, iteration  5094 loss:  1.0537788 acc:  0.38333333333333336\n",
      "New set of weights found, iteration  5173 loss:  1.0537219 acc:  0.4266666666666667\n",
      "New set of weights found, iteration  5176 loss:  1.0536375 acc:  0.38\n",
      "New set of weights found, iteration  5266 loss:  1.0535694 acc:  0.37666666666666665\n",
      "New set of weights found, iteration  5269 loss:  1.0533108 acc:  0.36666666666666664\n",
      "New set of weights found, iteration  5277 loss:  1.0532382 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  5280 loss:  1.0531307 acc:  0.38\n",
      "New set of weights found, iteration  5328 loss:  1.0529611 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  5333 loss:  1.0523667 acc:  0.39\n",
      "New set of weights found, iteration  5412 loss:  1.0522406 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  5512 loss:  1.052225 acc:  0.4\n",
      "New set of weights found, iteration  5830 loss:  1.0521476 acc:  0.4033333333333333\n",
      "New set of weights found, iteration  6027 loss:  1.0519933 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  6274 loss:  1.0519093 acc:  0.39\n",
      "New set of weights found, iteration  6289 loss:  1.051807 acc:  0.39\n",
      "New set of weights found, iteration  6334 loss:  1.0512271 acc:  0.39\n",
      "New set of weights found, iteration  6560 loss:  1.0511672 acc:  0.39\n",
      "New set of weights found, iteration  7332 loss:  1.0511596 acc:  0.4066666666666667\n",
      "New set of weights found, iteration  7366 loss:  1.0510243 acc:  0.4033333333333333\n",
      "New set of weights found, iteration  7424 loss:  1.0510125 acc:  0.4066666666666667\n",
      "New set of weights found, iteration  7550 loss:  1.0509754 acc:  0.4033333333333333\n",
      "New set of weights found, iteration  7634 loss:  1.0509197 acc:  0.41333333333333333\n",
      "New set of weights found, iteration  7705 loss:  1.0507858 acc:  0.41\n",
      "New set of weights found, iteration  7714 loss:  1.0506172 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  7887 loss:  1.0504193 acc:  0.41333333333333333\n",
      "New set of weights found, iteration  8657 loss:  1.0504012 acc:  0.4\n",
      "New set of weights found, iteration  9043 loss:  1.0503317 acc:  0.4033333333333333\n",
      "New set of weights found, iteration  10844 loss:  1.0502518 acc:  0.41\n",
      "New set of weights found, iteration  11111 loss:  1.0500691 acc:  0.4\n",
      "New set of weights found, iteration  11208 loss:  1.0500523 acc:  0.37666666666666665\n",
      "New set of weights found, iteration  11464 loss:  1.049916 acc:  0.3933333333333333\n",
      "New set of weights found, iteration  12283 loss:  1.0498258 acc:  0.37666666666666665\n",
      "New set of weights found, iteration  12589 loss:  1.0497069 acc:  0.36666666666666664\n",
      "New set of weights found, iteration  12592 loss:  1.0496517 acc:  0.41\n",
      "New set of weights found, iteration  12804 loss:  1.0496516 acc:  0.41333333333333333\n",
      "New set of weights found, iteration  12890 loss:  1.0495303 acc:  0.37333333333333335\n",
      "New set of weights found, iteration  13023 loss:  1.0493369 acc:  0.3933333333333333\n",
      "New set of weights found, iteration  13221 loss:  1.0491958 acc:  0.37666666666666665\n",
      "New set of weights found, iteration  13588 loss:  1.0490938 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  13823 loss:  1.049082 acc:  0.3933333333333333\n",
      "New set of weights found, iteration  14429 loss:  1.0489691 acc:  0.42333333333333334\n",
      "New set of weights found, iteration  16415 loss:  1.0488278 acc:  0.4\n",
      "New set of weights found, iteration  17770 loss:  1.0488268 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  18602 loss:  1.0487663 acc:  0.39\n",
      "New set of weights found, iteration  19024 loss:  1.0486704 acc:  0.4\n",
      "New set of weights found, iteration  19698 loss:  1.0486484 acc:  0.38333333333333336\n",
      "New set of weights found, iteration  22969 loss:  1.0486367 acc:  0.4\n",
      "New set of weights found, iteration  23467 loss:  1.0486046 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  23815 loss:  1.0485532 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  23983 loss:  1.0484079 acc:  0.4266666666666667\n",
      "New set of weights found, iteration  25107 loss:  1.0483462 acc:  0.4\n",
      "New set of weights found, iteration  25255 loss:  1.0482612 acc:  0.4033333333333333\n",
      "New set of weights found, iteration  25948 loss:  1.0482272 acc:  0.42333333333333334\n",
      "New set of weights found, iteration  26918 loss:  1.0480641 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  27245 loss:  1.0479144 acc:  0.3933333333333333\n",
      "New set of weights found, iteration  27476 loss:  1.047883 acc:  0.43\n",
      "New set of weights found, iteration  27610 loss:  1.0477638 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  31127 loss:  1.0474634 acc:  0.4066666666666667\n",
      "New set of weights found, iteration  39036 loss:  1.0474485 acc:  0.41\n",
      "New set of weights found, iteration  41328 loss:  1.0472922 acc:  0.3933333333333333\n",
      "New set of weights found, iteration  43485 loss:  1.0472406 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  43573 loss:  1.0472084 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  44036 loss:  1.0471612 acc:  0.38\n",
      "New set of weights found, iteration  44135 loss:  1.0470983 acc:  0.41333333333333333\n",
      "New set of weights found, iteration  44279 loss:  1.0470508 acc:  0.41\n",
      "New set of weights found, iteration  44606 loss:  1.0470152 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  44677 loss:  1.0470103 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  45186 loss:  1.0469847 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  45519 loss:  1.0468905 acc:  0.3933333333333333\n",
      "New set of weights found, iteration  45696 loss:  1.0468715 acc:  0.41333333333333333\n",
      "New set of weights found, iteration  46890 loss:  1.0468358 acc:  0.4033333333333333\n",
      "New set of weights found, iteration  47527 loss:  1.046746 acc:  0.4033333333333333\n",
      "New set of weights found, iteration  48733 loss:  1.0466479 acc:  0.4\n",
      "New set of weights found, iteration  48843 loss:  1.0464959 acc:  0.4\n",
      "New set of weights found, iteration  48926 loss:  1.0463885 acc:  0.38333333333333336\n",
      "New set of weights found, iteration  48994 loss:  1.0462117 acc:  0.41333333333333333\n",
      "New set of weights found, iteration  52767 loss:  1.0461845 acc:  0.43333333333333335\n",
      "New set of weights found, iteration  53364 loss:  1.0461417 acc:  0.4\n",
      "New set of weights found, iteration  53586 loss:  1.0461249 acc:  0.4\n",
      "New set of weights found, iteration  53610 loss:  1.0460458 acc:  0.42333333333333334\n",
      "New set of weights found, iteration  53840 loss:  1.0458786 acc:  0.41\n",
      "New set of weights found, iteration  54113 loss:  1.045711 acc:  0.4033333333333333\n",
      "New set of weights found, iteration  62720 loss:  1.0456356 acc:  0.42\n",
      "New set of weights found, iteration  63277 loss:  1.0455959 acc:  0.4066666666666667\n",
      "New set of weights found, iteration  63771 loss:  1.0455804 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  64038 loss:  1.0454959 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  65016 loss:  1.0453753 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  65307 loss:  1.04535 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  66519 loss:  1.045299 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  67847 loss:  1.0450583 acc:  0.4033333333333333\n",
      "New set of weights found, iteration  68306 loss:  1.0449746 acc:  0.4166666666666667\n",
      "New set of weights found, iteration  71286 loss:  1.0448848 acc:  0.43666666666666665\n",
      "New set of weights found, iteration  73578 loss:  1.0448807 acc:  0.41333333333333333\n",
      "New set of weights found, iteration  73785 loss:  1.044876 acc:  0.41333333333333333\n",
      "New set of weights found, iteration  73986 loss:  1.0448445 acc:  0.4266666666666667\n",
      "New set of weights found, iteration  74099 loss:  1.0447749 acc:  0.41\n",
      "New set of weights found, iteration  74886 loss:  1.0447526 acc:  0.4\n",
      "New set of weights found, iteration  74948 loss:  1.0447204 acc:  0.4066666666666667\n",
      "New set of weights found, iteration  75025 loss:  1.0446256 acc:  0.39\n",
      "New set of weights found, iteration  75309 loss:  1.0446205 acc:  0.42\n",
      "New set of weights found, iteration  81251 loss:  1.0445701 acc:  0.42333333333333334\n",
      "New set of weights found, iteration  81864 loss:  1.0445228 acc:  0.38666666666666666\n",
      "New set of weights found, iteration  82309 loss:  1.0444576 acc:  0.4033333333333333\n",
      "New set of weights found, iteration  82776 loss:  1.0444316 acc:  0.39666666666666667\n",
      "New set of weights found, iteration  82874 loss:  1.0443579 acc:  0.43333333333333335\n",
      "New set of weights found, iteration  83519 loss:  1.0442119 acc:  0.42333333333333334\n",
      "New set of weights found, iteration  83786 loss:  1.0440992 acc:  0.4033333333333333\n",
      "New set of weights found, iteration  84955 loss:  1.0439862 acc:  0.39666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Try with sprial dataset instead\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "# create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create model\n",
    "dense1 = Layer_Dense(2, 3)      # First dense layer, 2 inputs, 3 outputs\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3, 3)      # Seconde dense layer, 3 inputs, 3 outputs\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Helper variables\n",
    "lowest_loss = 99999999 # some initial value\n",
    "best_dense1_weights = dense1.weights.copy()\n",
    "best_dense1_biases = dense1.biases.copy()\n",
    "best_dense2_weights = dense2.weights.copy()\n",
    "best_dense2_biases = dense2.biases.copy()\n",
    "\n",
    "for iteration in range(100000):\n",
    "    \n",
    "    # Generate a new set of weights for iteration\n",
    "    dense1.weights += 0.05 * np.random.randn(2, 3)\n",
    "    dense1.biases += 0.05 * np.random.randn(1, 3)\n",
    "    dense2.weights += 0.05 * np.random.randn(3, 3)\n",
    "    dense2.biases += 0.05 * np.random.randn(1, 3)\n",
    "    \n",
    "    # Perform a forward pass of the training data through this layer\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # it takes the output of second dense layer here and returns loss\n",
    "    loss = loss_function.calculate(activation2.output, y)\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # Calculate values along first axis\n",
    "    predictions = np.argmax(activation2.output, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    # if los is smaller - print and save weights and biases aside\n",
    "    if loss < lowest_loss:\n",
    "        print(\"New set of weights found, iteration \", iteration,\n",
    "                  \"loss: \", loss,\n",
    "                  \"acc: \", accuracy)\n",
    "        best_dense1_weights = dense1.weights.copy()\n",
    "        best_dense1_biases = dense1.biases.copy()\n",
    "        best_dense2_weights = dense2.weights.copy()\n",
    "        best_dense2_biases = dense2.biases.copy()\n",
    "        lowest_loss = loss\n",
    "# Revert weights and biases\n",
    "    else:\n",
    "        dense1.weights = best_dense1_weights.copy()\n",
    "        dense1.biases = best_dense1_biases.copy()\n",
    "        dense2.weights = best_dense2_weights.copy()\n",
    "        dense2.biases = best_dense2_biases.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training sessions ended with almost no progress. Loss decreased slightly and accuracy is barely above the initial values. Leter we will learn that the most probably reason for this is called a *local mimimum* of loss. The data complexity is also not irrelevant here. It seems like hard problems are hard for a reason, we need to approach this problem in a more intelligent way. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
