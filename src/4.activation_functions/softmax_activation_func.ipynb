{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax Activation Function**  \n",
    "For our model we want it to be a classifier, so we need a activaion function meant for classification. One of these is the Softmax activation function. Why another activation function? In this case the rectified linear unit is unbounded, not normalized with other units and exclusive. Not norlmalized means a number or value can be anything, an output of [23, 87, 220] is without context and exclusive means each output is independet of each other.  \n",
    "  \n",
    "To adress this lack of context, the softmax activation on output data can take non-normalized (uncalibrated) inputs and produce a normalized distribution of probabilities for our classes. Thedistribution returned by the softwax activation function represents a confidence score for each class and will add upp to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function: S(i,j) = e^z(i,j) / sum e^z(i,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [4.8, 1.21, 2.385]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to exponentiate the outputs. We do this with Eulers constant *e* to the power of the given parameter: y = e^x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values:\n",
      "[121.51041751873483, 3.353484652549023, 10.859062664920513]\n"
     ]
    }
   ],
   "source": [
    "from numpy import math\n",
    "\n",
    "# values from previous steps, when describing what a neural network is\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "# e -mathematical constant\n",
    "math.e\n",
    "\n",
    "# For each value in a vector, calculate the the exponential value\n",
    "exp_values = []\n",
    "for output in layer_outputs:\n",
    "    exp_values.append(math.e ** output)\n",
    "print('exponentiated values:')\n",
    "print(exp_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponentiation serves multiple purposes. For calculating probabilities, we need non-negative values. Take for example output [4.8, 1.21, -2.385], even after normalization, the last value will still be negative wince we just divide all of them by their sum. A negative probability (or confidence) does not make sense to us. An exponential value of any number is always non-negative - it returns 0 for negative infinity, 1 for the input of 0, and increases for positive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1.0], [1, 2.718281828459045], [2, 7.3890560989306495], [3, 20.085536923187664], [4, 54.59815003314423]]\n"
     ]
    }
   ],
   "source": [
    "# OMA - Exponential function y = e^x\n",
    "\n",
    "# some random x values\n",
    "values = [0,1,2,3,4]\n",
    "\n",
    "# create empty list\n",
    "dat = []\n",
    "\n",
    "for value in values:\n",
    "    d = [value, math.e**value]\n",
    "    dat.append(d)\n",
    "\n",
    "print(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized exponentiated values:\n",
      "[0.8952826639572619, 0.024708306782099374, 0.0800090292606387]\n",
      "Sum of normalized values: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Next normalize values\n",
    "norm_base = sum(exp_values)\n",
    "norm_values = []\n",
    "for value in exp_values:\n",
    "    norm_values.append(value / norm_base)\n",
    "print('Normalized exponentiated values:')\n",
    "print(norm_values)\n",
    "\n",
    "print('Sum of normalized values:', sum(norm_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform the same operation using NumPy accordingly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values:\n",
      "[121.51041752   3.35348465  10.85906266]\n",
      "normalized exponentiated values:\n",
      "[0.89528266 0.02470831 0.08000903]\n",
      "sum of normalized values: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Values from the eariler when described what a neural network is\n",
    "layer_outpus = [4.8, 1.21, 2.385]\n",
    "\n",
    "# For each value in a vector, calculate exponential value\n",
    "exp_values = np.exp(layer_outputs)\n",
    "print('exponentiated values:')\n",
    "print(exp_values)\n",
    "\n",
    "# Now normalize values\n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "print('normalized exponentiated values:')\n",
    "print(norm_values)\n",
    "print('sum of normalized values:', np.sum(norm_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the result are similar but faster and easier to read with numpy. We can exponentiate all of the values with a single call of np.exp(), then immediately normalize them with the sum. To train in batches, we must convert this functionality to accept layer outputs in batches. That is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.71828183   7.3890561   20.08553692  12.18249396]\n",
      " [  7.3890561  148.4131591    0.36787944   7.3890561 ]\n",
      " [  0.22313016 148.4131591    0.36787944   0.44932896]]\n",
      "[[0.06414769 0.17437149 0.47399085 0.28748998]\n",
      " [0.04517666 0.90739747 0.00224921 0.04517666]\n",
      " [0.00149297 0.99303905 0.0024615  0.00300648]]\n"
     ]
    }
   ],
   "source": [
    "# re-create inputs first\n",
    "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 5.0, -1.0, -0.8]]\n",
    "\n",
    "# Get unnormalized probilities\n",
    "exp_values = np.exp(inputs)\n",
    "\n",
    "# Normalize them for each sample\n",
    "probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims=True)\n",
    "# in a 2D array or matrix axis = 0 refers to rows and axis = 1 refers to columns\n",
    "# keepdim\n",
    "\n",
    "print(exp_values)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding *axis = 1* above. In a 2D array or matrix axis = 0 refers to rows and axis = 1 refers to columns. An example of how axis affects the sum in numpy, use first the default which is *None*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum without axis\n",
      "17.162\n",
      "This will be identical to the above since default is Non:\n",
      "17.162\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "layer_outputs = np.array([[4.8, 1.1, 2.385],\n",
    "                         [8.0, -1.81, 0.2],\n",
    "                         [1.41, 1.051, 0.026]])\n",
    "\n",
    "print('Sum without axis')\n",
    "print(np.sum(layer_outputs))\n",
    "\n",
    "print('This will be identical to the above since default is Non:')\n",
    "print(np.sum(layer_outputs, axis=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, without axis argument specified numpy will sum all the values even if they are in varying dimensions. Next, *axis=0*. means we sum row-wise, along axis 0. I.e the output has the same size as this axis, as at each of the positoin of this output, the values from all other dimensions at this points are summed to form it. In this case a 2D array, where we have only a single other dimensions, the columns, the output vector will sum these columns. This means we perform (4.8 + 8.0 + 1.41) etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another way to think of it w/ a matrix == axis 0: columns:\n",
      "[14.21   0.341  2.611]\n"
     ]
    }
   ],
   "source": [
    "print('Another way to think of it w/ a matrix == axis 0: columns:')\n",
    "print(np.sum(layer_outputs, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not what we want. We want sums of the rows. We want sums of rows. Before we show hot to do it in numpy we do it also from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But we want to sum the rows instead, like this w/ raw py:\n",
      "8.285\n",
      "6.39\n",
      "2.4869999999999997\n"
     ]
    }
   ],
   "source": [
    "print('But we want to sum the rows instead, like this w/ raw py:')\n",
    "\n",
    "for i in layer_outputs:\n",
    "    print(sum(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could take above and append these numbers to some list in any way we want. But we till use numpy and sum along axis 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sw we can use sum axis 1, but note the current shape:\n",
      "[8.285 6.39  2.487]\n"
     ]
    }
   ],
   "source": [
    "print('Sw we can use sum axis 1, but note the current shape:')\n",
    "print(np.sum(layer_outputs, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With above we did get the sums we wanted, but we want to simplify the outputs to a single value per sample.We´re tring to sum all the outputs from a layer for each sample in a batch, converting they layer´s output array with row length equal to the number of neurons in the layer, to just one value. We need a column vector with these values since it will let us normalize the entire batch of samples, sample-wise, with a single calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum axis 1, but keep the same dimensions as input:\n",
      "[[8.285]\n",
      " [6.39 ]\n",
      " [2.487]]\n"
     ]
    }
   ],
   "source": [
    "print('Sum axis 1, but keep the same dimensions as input:')\n",
    "print(np.sum(layer_outputs, axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the *keepdims=True* we keep the same dimensions as the input. Now, if we divide the array containing a batch of the outpus with this array, numpy will perform this sample-wise. This means numpy will divide all of the values from each output row by the corresponding row from the sum array. Since this sum in each row is a single value, it will be used to divide every vvalue from the corresponding output row. Now we combine this into a softmax class::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                           keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                           keepdims=True)\n",
    "        \n",
    "        self.output = probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we included a subtraction of the largest of the inputs before we did the exponentiation.  \n",
    "  \n",
    "There are two main pervasive challenges with neural networks: 'dead neurons' and very large numbers (referred to as 'exploding' values). Dead neurons and enormouse numbers can cause havoc down the line and render a network useless over time. The exponential function used in softmax activation is one of the sources of exploding values. Let´s see some examples of how and why this can happen easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.718281828459045\n",
      "22026.465794806718\n",
      "2.6881171418161356e+43\n",
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-cc78b0df7c35>:9: RuntimeWarning: overflow encountered in exp\n",
      "  print(np.exp(1000))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.exp(1))\n",
    "\n",
    "print(np.exp(10))\n",
    "\n",
    "print(np.exp(100))\n",
    "\n",
    "print(np.exp(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see it does not take a very large number, only 1000, to cause an overflow error. We know the exponential function tends towards 0 as its input value approaches negative infinity, and output is 1 when the input is 0 (as shown in the chart earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.exp(-np.inf), np.exp(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this property to prevent the exponential function from overflowing. With Softmax, thanks to the normalization, we can subtract any value from all of the inputs, and iit will not change the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09003057 0.24472847 0.66524096]]\n"
     ]
    }
   ],
   "source": [
    "softmax = Activation_Softmax()\n",
    "\n",
    "softmax.forward([[1, 2, 3]])\n",
    "print(softmax.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09003057 0.24472847 0.66524096]]\n"
     ]
    }
   ],
   "source": [
    "softmax = Activation_Softmax() # subtracted 3 - max from the list\n",
    "\n",
    "softmax.forward([[-2, -1, 0]])\n",
    "print(softmax.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we divide the data [1, 2, 3] with e.g 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18632372 0.30719589 0.50648039]]\n"
     ]
    }
   ],
   "source": [
    "softmax = Activation_Softmax()\n",
    "\n",
    "softmax.forward([[0.5, 1, 1.5]])\n",
    "print(softmax.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output confidences have changed due to the nonlinearity nature of the exponentiation. This is one reason why we need to scale all of the input data to a neural network in the same way which we will explain in further detail in chaptere 22.  \n",
    "  \n",
    "Now, we add another dense layer as the output layer, setting it to contain as many inputs as the previous layer has outputs and as many outputs as our data includes classes. Then we apply softmax activation to the output of this new layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# Dense layer class\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # initialization of weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    " \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# ReLu activation function\n",
    "class Activation_ReLU:\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        # calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs) \n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,keepdims=True)\n",
    "        \n",
    "        self.output = probabilities        \n",
    "        \n",
    "        \n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLu activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output)\n",
    "# of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Make a forward pass thgouth activation function\n",
    "# it takes the output of the first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Layer_Dense' object has no attribute 'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-5efd54758219>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Make a forward pass thgouth activation function it takes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# the output of second dense layer here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mactivation2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdense2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Lets see the output of the first few samples:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Layer_Dense' object has no attribute 'output'"
     ]
    }
   ],
   "source": [
    "# Make a forward pass thgouth activation function it takes\n",
    "# the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Lets see the output of the first few samples:\n",
    "print(activation2.output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouble shoot above..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
