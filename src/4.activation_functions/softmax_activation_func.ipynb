{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax Activation Function**  \n",
    "For our model we want it to be a classifier, so we need a activaion function meant for classification. One of these is the Softmax activation function. Why another activation function? In this case the rectified linear unit is unbounded, not normalized with other units and exclusive. Not norlmalized means a number or value can be anything, an output of [23, 87, 220] is without context and exclusive means each output is independet of each other.  \n",
    "  \n",
    "To adress this lack of context, the softmax activation on output data can take non-normalized (uncalibrated) inputs and produce a normalized distribution of probabilities for our classes. Thedistribution returned by the softwax activation function represents a confidence score for each class and will add upp to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function: S(i,j) = e^z(i,j) / sum e^z(i,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [4.8, 1.21, 2.385]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to exponentiate the outputs. We do this with Eulers constant *e* to the power of the given parameter: y = e^x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values:\n",
      "[121.51041751873483, 3.353484652549023, 10.859062664920513]\n"
     ]
    }
   ],
   "source": [
    "from numpy import math\n",
    "\n",
    "# values from previous steps, when describing what a neural network is\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "# e -mathematical constant\n",
    "math.e\n",
    "\n",
    "# For each value in a vector, calculate the the exponential value\n",
    "exp_values = []\n",
    "for output in layer_outputs:\n",
    "    exp_values.append(math.e ** output)\n",
    "print('exponentiated values:')\n",
    "print(exp_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponentiation serves multiple purposes. For calculating probabilities, we need non-negative values. Take for example output [4.8, 1.21, -2.385], even after normalization, the last value will still be negative wince we just divide all of them by their sum. A negative probability (or confidence) does not make sense to us. An exponential value of any number is always non-negative - it returns 0 for negative infinity, 1 for the input of 0, and increases for positive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1.0], [1, 2.718281828459045], [2, 7.3890560989306495], [3, 20.085536923187664], [4, 54.59815003314423]]\n"
     ]
    }
   ],
   "source": [
    "# OMA - Exponential function y = e^x\n",
    "\n",
    "# some random x values\n",
    "values = [0,1,2,3,4]\n",
    "\n",
    "# create empty list\n",
    "dat = []\n",
    "\n",
    "for value in values:\n",
    "    d = [value, math.e**value]\n",
    "    dat.append(d)\n",
    "\n",
    "print(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized exponentiated values:\n",
      "[0.8952826639572619, 0.024708306782099374, 0.0800090292606387]\n",
      "Sum of normalized values: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Next normalize values\n",
    "norm_base = sum(exp_values)\n",
    "norm_values = []\n",
    "for value in exp_values:\n",
    "    norm_values.append(value / norm_base)\n",
    "print('Normalized exponentiated values:')\n",
    "print(norm_values)\n",
    "\n",
    "print('Sum of normalized values:', sum(norm_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform the same operation using NumPy accordingly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values:\n",
      "[121.51041752   3.35348465  10.85906266]\n",
      "normalized exponentiated values:\n",
      "[0.89528266 0.02470831 0.08000903]\n",
      "sum of normalized values: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Values from the eariler when described what a neural network is\n",
    "layer_outpus = [4.8, 1.21, 2.385]\n",
    "\n",
    "# For each value in a vector, calculate exponential value\n",
    "exp_values = np.exp(layer_outputs)\n",
    "print('exponentiated values:')\n",
    "print(exp_values)\n",
    "\n",
    "# Now normalize values\n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "print('normalized exponentiated values:')\n",
    "print(norm_values)\n",
    "print('sum of normalized values:', np.sum(norm_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the reslt are similar but faster and easier to read with numpy. We can exponentiate all of the values with a single call of np.exp(), then immediately normalize them with the sum. To train in batches, we must convert this functionality to accept layer outputs in batches. That is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.71828183   7.3890561   20.08553692  12.18249396]\n",
      " [  7.3890561  148.4131591    0.36787944   7.3890561 ]\n",
      " [  0.22313016 148.4131591    0.36787944   0.44932896]]\n",
      "[[0.06414769 0.17437149 0.47399085 0.28748998]\n",
      " [0.04517666 0.90739747 0.00224921 0.04517666]\n",
      " [0.00149297 0.99303905 0.0024615  0.00300648]]\n"
     ]
    }
   ],
   "source": [
    "# re-create inputs first\n",
    "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 5.0, -1.0, -0.8]]\n",
    "\n",
    "# Get unnormalized probilities\n",
    "exp_values = np.exp(inputs)\n",
    "\n",
    "# Normalize them for each sample\n",
    "probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims=True)\n",
    "# in a 2D array or matrix axis = 0 refers to rows and axis = 1 refers to columns\n",
    "# keepdim\n",
    "\n",
    "print(exp_values)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding *axis = 1* above. In a 2D array or matrix axis = 0 refers to rows and axis = 1 refers to columns. An example of how axis affects the sum in numpy, use first the default which is *None*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum without axis\n",
      "17.162\n",
      "This will be identical to the above since default is Non:\n",
      "17.162\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "layer_outputs = np.array([[4.8, 1.1, 2.385],\n",
    "                         [8.0, -1.81, 0.2],\n",
    "                         [1.41, 1.051, 0.026]])\n",
    "\n",
    "print('Sum without axis')\n",
    "print(np.sum(layer_outputs))\n",
    "\n",
    "print('This will be identical to the above since default is Non:')\n",
    "print(np.sum(layer_outputs, axis=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, without axis argument specified numpy will sum all the values even if they are in varying dimensions. Next, *axis=0*. means we sum row-wise, along axis 0. I.e the output has the same size as this axis, as at each of the positoin of this output, the values from all other dimensions at this points are summed to form it. In this case a 2D array, where we have only a single other dimensions, the columns, the output vector will sum these columns. This means we perform (4.8 + 8.0 + 1.41) etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another way to think of it w/ a matrix == axis 0: columns:\n",
      "[14.21   0.341  2.611]\n"
     ]
    }
   ],
   "source": [
    "print('Another way to think of it w/ a matrix == axis 0: columns:')\n",
    "print(np.sum(layer_outputs, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not what we want. We want sums of the rows. We want sums of rows. Before we show hot to do it in numpy we do it also from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But we want to sum the rows instead, like this w/ raw py:\n",
      "8.285\n",
      "6.39\n",
      "2.4869999999999997\n"
     ]
    }
   ],
   "source": [
    "print('But we want to sum the rows instead, like this w/ raw py:')\n",
    "\n",
    "for i in layer_outputs:\n",
    "    print(sum(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could take above and append these numbers to some list in any way we want. But we till use numpy and sum along axis 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sw we can use sum axis 1, but note the current shape:\n",
      "[8.285 6.39  2.487]\n"
     ]
    }
   ],
   "source": [
    "print('Sw we can use sum axis 1, but note the current shape:')\n",
    "print(np.sum(layer_outputs, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With above we did get the sums we wanted, but we want to simplify the outputs to a single value per sample.We´re tring to sum all the outputs from a layer for each sample in a batch, converting they layer´s output array with row length equal to the number of neurons in the layer, to just one value. We need a column vector with these values since it will let us normalize the entire batch of samples, sample-wise, with a single calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum axis 1, but keep the same dimensions as input:\n",
      "[[8.285]\n",
      " [6.39 ]\n",
      " [2.487]]\n"
     ]
    }
   ],
   "source": [
    "print('Sum axis 1, but keep the same dimensions as input:')\n",
    "print(np.sum(layer_outputs, axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
