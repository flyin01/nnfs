{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rectified Linear Unit (ReLU) activation function**  \n",
    "The ReLU activation function is relatively easy to code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "# alt 1\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "\n",
    "output = []\n",
    "for i in inputs:\n",
    "    if i > 0:\n",
    "        output.append(i)\n",
    "    else:\n",
    "        output.append(0)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs was a made up list of starting values. The ReLU in this code is a loop that checks if current value is greater than 0. If yes, then we append the current value to the list, if not we append 0. This can be written in a simpler for, we just need to take the largets of the two values: 0 or neuron values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "# alt 2\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "\n",
    "output = []\n",
    "for i in inputs:\n",
    "    output.append(max(0,i))\n",
    "    \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy has equivalent function in np.maximum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "output = np.maximum(0, inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numpy method compares each element of the input list (or array) and returns and object of the same shape filled with new values. This we will use in the rectified linear activation class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu activation function\n",
    "class Activation_ReLU:\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        # calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply this activation function class to the dense layerÂ´s output in our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as  np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer class\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # initialization of weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    " \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # calcualte output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# ReLu activation function\n",
    "class Activation_ReLU:\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        # calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.        ]\n",
      " [0.         0.00011395 0.        ]\n",
      " [0.         0.00031729 0.        ]\n",
      " [0.         0.00052666 0.        ]\n",
      " [0.         0.00071401 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes = 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2,3)\n",
    "\n",
    "# Create activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Make a forward pass of the training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Forward pass throught activation function\n",
    "# Takes in output from previous layer\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Check the output of the first few samples\n",
    "print(activation1.output[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the negative values have been cut off and modified to zero. That sums up the activation function used in the hidden layer. Next, we will talk about the activation function that we will use on the output of the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.        ]\n",
      " [ 0.00299556  0.00964661]\n",
      " [ 0.01288097  0.01556285]\n",
      " [ 0.02997479  0.0044481 ]\n",
      " [ 0.03931246  0.00932828]\n",
      " [ 0.00082883  0.05049825]\n",
      " [ 0.05348352  0.02850628]\n",
      " [ 0.0417362   0.05707521]\n",
      " [ 0.05546339  0.05876868]\n",
      " [ 0.08160383  0.04006591]\n",
      " [ 0.08918751  0.0474197 ]\n",
      " [ 0.10716084 -0.02936382]\n",
      " [ 0.1211832  -0.00264751]\n",
      " [ 0.12877773  0.02567947]\n",
      " [ 0.14111297 -0.00922449]\n",
      " [ 0.15057947 -0.01681263]\n",
      " [ 0.11347637 -0.11507779]\n",
      " [ 0.17155251 -0.00751816]\n",
      " [ 0.16718684 -0.07145914]\n",
      " [ 0.19132587  0.01507932]]\n"
     ]
    }
   ],
   "source": [
    "# check input data\n",
    "print(X[:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
